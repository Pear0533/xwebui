[{"id":"comfyui_i2i_universal_pipe","user_id":"1253ad24-54ec-4e73-9892-1b7fa1e5a06a","name":"Image Edit","type":"pipe","content":"import json\nimport uuid\nimport aiohttp\nimport asyncio\nimport random\nfrom typing import List, Dict, Callable, Optional\nfrom pydantic import BaseModel, Field\nfrom open_webui.utils.misc import get_last_user_message_item  # type: ignore\nfrom open_webui.utils.chat import generate_chat_completion  # type: ignore\nfrom open_webui.models.users import User, Users\n\nfrom open_webui.constants import TASKS\nimport logging\nimport requests\n\nimport io\nimport mimetypes\nfrom fastapi import UploadFile\nfrom open_webui.routers.files import upload_file_handler\n\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef get_loaded_models(api_url: str = \"http://localhost:11434\") -> list[str]:\n    try:\n        response = requests.get(f\"{api_url.rstrip('/')}/api/ps\", timeout=5)\n        response.raise_for_status()\n        return response.json().get(\"models\", [])\n    except requests.RequestException as e:\n        logger.error(f\"Error fetching loaded Ollama models: {e}\")\n        return []\n\n\ndef unload_all_models(api_url: str = \"http://localhost:11434\"):\n    try:\n        for model in get_loaded_models(api_url):\n            model_name = model.get(\"name\")\n            if model_name:\n                requests.post(\n                    f\"{api_url.rstrip('/')}/api/generate\",\n                    json={\"model\": model_name, \"keep_alive\": 0},\n                    timeout=10,\n                )\n    except requests.RequestException as e:\n        logger.error(f\"Error unloading Ollama models: {e}\")\n\n\nDEFAULT_WORKFLOW_JSON = json.dumps(\n    {\n        \"6\": {\n            \"inputs\": {\n                \"text\": \"re imagine this man maintainig the facial features as a medieval fantasy king...\",\n                \"clip\": [\"38\", 0],\n            },\n            \"class_type\": \"CLIPTextEncode\",\n        },\n        \"35\": {\n            \"inputs\": {\"guidance\": 2.5, \"conditioning\": [\"177\", 0]},\n            \"class_type\": \"FluxGuidance\",\n        },\n        \"37\": {\n            \"inputs\": {\n                \"unet_name\": \"flux-kontext/flux1-dev-kontext_fp8_scaled.safetensors\",\n                \"weight_dtype\": \"fp8_e4m3fn_fast\",\n            },\n            \"class_type\": \"UNETLoader\",\n        },\n        \"38\": {\n            \"inputs\": {\n                \"clip_name1\": \"clip_l.safetensors\",\n                \"clip_name2\": \"flux/t5xxl_fp8_e4m3fn_scaled.safetensors\",\n                \"type\": \"flux\",\n                \"device\": \"cpu\",\n            },\n            \"class_type\": \"DualCLIPLoader\",\n        },\n        \"39\": {\n            \"inputs\": {\"vae_name\": \"Flux/ae.safetensors\"},\n            \"class_type\": \"VAELoader\",\n        },\n        \"42\": {\"inputs\": {\"image\": [\"196\", 0]}, \"class_type\": \"FluxKontextImageScale\"},\n        \"124\": {\n            \"inputs\": {\"pixels\": [\"42\", 0], \"vae\": [\"39\", 0]},\n            \"class_type\": \"VAEEncode\",\n        },\n        \"135\": {\n            \"inputs\": {\"conditioning\": [\"6\", 0]},\n            \"class_type\": \"ConditioningZeroOut\",\n        },\n        \"136\": {\n            \"inputs\": {\"filename_prefix\": \"owui/owui\", \"images\": [\"194\", 5]},\n            \"class_type\": \"SaveImage\",\n        },\n        \"177\": {\n            \"inputs\": {\"conditioning\": [\"6\", 0], \"latent\": [\"124\", 0]},\n            \"class_type\": \"ReferenceLatent\",\n        },\n        \"194\": {\n            \"inputs\": {\n                \"seed\": 558680250753563,\n                \"steps\": 20,\n                \"cfg\": 1,\n                \"sampler_name\": \"dpmpp_2m\",\n                \"scheduler\": \"beta\",\n                \"denoise\": 1,\n                \"preview_method\": \"none\",\n                \"vae_decode\": \"true (tiled)\",\n                \"model\": [\"37\", 0],\n                \"positive\": [\"35\", 0],\n                \"negative\": [\"135\", 0],\n                \"latent_image\": [\"124\", 0],\n                \"optional_vae\": [\"39\", 0],\n            },\n            \"class_type\": \"KSampler (Efficient)\",\n        },\n        \"196\": {\"inputs\": {\"base64_data\": \"\"}, \"class_type\": \"ETN_LoadImageBase64\"},\n    },\n    indent=2,\n)\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        # ... (this section remains unchanged)\n        ComfyUI_Address: str = Field(\n            default=\"http://127.0.0.1:8188\",\n            description=\"Address of the running ComfyUI server.\",\n        )\n        ComfyUI_Workflow_JSON: str = Field(\n            default=DEFAULT_WORKFLOW_JSON,\n            description=\"The entire ComfyUI workflow in JSON format.\",\n            extra={\"type\": \"textarea\"},\n        )\n        Prompt_Node_ID: str = Field(\n            default=\"6\", description=\"The ID of the node that accepts the text prompt.\"\n        )\n        Image_Node_ID: str = Field(\n            default=\"196\",\n            description=\"The ID of the node that accepts the Base64 image.\",\n        )\n        Seed_Node_ID: str = Field(\n            default=\"194\",\n            description=\"The ID of the sampler node to apply a random seed to.\",\n        )\n        enhance_prompt: bool = Field(\n            default=False, description=\"Use vision model to enhance prompt\"\n        )\n        vision_model_id: str = Field(\n            default=\"\", description=\"Vision model to be used as prompt enhancer\"\n        )\n        enhancer_system_prompt: str = Field(\n            default=\"\"\"\n            You are a visual prompt engineering assistant. \n            For each request, you will receive a user-provided prompt and an image to be edited. \n            Carefully analyze the image’s content (objects, colors, environment, style, mood, etc.) along with the user’s intent. \n            Then generate a single, improved editing prompt for the FLUX Kontext model using best practices. \n            Be specific and descriptive: use exact color names and detailed adjectives, and use clear action verbs like “change,” “add,” or “remove.” \n            Name each subject explicitly (for example, “the woman with short black hair,” “the red sports car”), avoiding pronouns like “her” or “it.” \n            Include relevant details from the image. \n            Preserve any elements the user did not want changed by stating them explicitly (for example, “keep the same composition and lighting”). \n            If the user wants to add or change any text, put the exact words in quotes (for example, replace “joy” with “BFL”).\n            Focus only on editing instructions. \n            Finally, output only the final enhanced prompt (the refined instruction) with no additional explanation or commentary.\n            \"\"\",\n            description=\"System prompt to be used on the prompt enhancement process\",\n        )\n        unload_ollama_models: bool = Field(\n            default=False,\n            description=\"Unload all Ollama models from VRAM before running.\",\n        )\n        ollama_url: str = Field(\n            default=\"http://host.docker.internal:11434\",\n            description=\"Ollama API URL for unloading models.\",\n        )\n        max_wait_time: int = Field(\n            default=1200, description=\"Max wait time for generation (seconds).\"\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.client_id = str(uuid.uuid4())\n\n    def _save_image_and_get_public_url(\n        self, request, image_data: bytes, content_type: str, user: User\n    ) -> str:\n        \"\"\"\n        Saves the image data to OpenWebUI's file storage and returns a publicly accessible URL.\n        This logic is adapted from OpenWebUI's native image generation handling.\n        \"\"\"\n        try:\n            image_format = mimetypes.guess_extension(content_type)\n            if not image_format:\n\n                image_format = \".png\"\n\n            file = UploadFile(\n                file=io.BytesIO(image_data),\n                filename=f\"generated-image{image_format}\",\n                headers={\"content-type\": content_type},\n            )\n\n            file_item = upload_file_handler(\n                request=request,\n                file=file,\n                metadata={},\n                process=False,\n                user=user,\n            )\n\n            url = request.app.url_path_for(\"get_file_content_by_id\", id=file_item.id)\n            return url\n        except Exception as e:\n            logger.error(f\"Error saving image to OpenWebUI: {e}\", exc_info=True)\n            raise\n\n    async def emit_status(\n        self, event_emitter: Callable, level: str, description: str, done: bool = False\n    ):\n        if event_emitter:\n            await event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": description,\n                        \"done\": done,\n                    },\n                }\n            )\n\n    async def wait_for_job_signal(\n        self, ws_api_url: str, prompt_id: str, event_emitter: Callable\n    ) -> bool:\n        \"\"\"Waits for the 'executed' signal from WebSocket without fetching data.\"\"\"\n        start_time = asyncio.get_event_loop().time()\n        try:\n            async with aiohttp.ClientSession().ws_connect(\n                f\"{ws_api_url}?clientId={self.client_id}\", timeout=30\n            ) as ws:\n                async for msg in ws:\n                    if (\n                        asyncio.get_event_loop().time() - start_time\n                        > self.valves.max_wait_time\n                    ):\n                        raise TimeoutError(\n                            f\"WebSocket wait timed out after {self.valves.max_wait_time}s\"\n                        )\n                    if msg.type != aiohttp.WSMsgType.TEXT:\n                        continue\n                    message = json.loads(msg.data)\n                    msg_type, data = message.get(\"type\"), message.get(\"data\", {})\n\n                    if msg_type == \"status\":\n                        q_remaining = (\n                            data.get(\"status\", {})\n                            .get(\"exec_info\", {})\n                            .get(\"queue_remaining\", 0)\n                        )\n                        await self.emit_status(\n                            event_emitter,\n                            \"info\",\n                            f\"In queue... {q_remaining} tasks remaining.\",\n                        )\n                    elif msg_type == \"progress\":\n                        progress = int(data.get(\"value\", 0) / data.get(\"max\", 1) * 100)\n                        await self.emit_status(\n                            event_emitter, \"info\", f\"Processing... {progress}%\"\n                        )\n                    elif msg_type == \"executed\" and data.get(\"prompt_id\") == prompt_id:\n                        logger.info(\n                            f\"Execution signal received for prompt {prompt_id}.\"\n                        )\n                        return True\n                    elif (\n                        msg_type == \"execution_error\"\n                        and data.get(\"prompt_id\") == prompt_id\n                    ):\n                        raise Exception(\n                            f\"ComfyUI Error: {data.get('exception_message', 'Unknown error')}\"\n                        )\n        except asyncio.TimeoutError:\n            raise TimeoutError(\n                f\"Operation timed out after {self.valves.max_wait_time}s\"\n            )\n        except Exception as e:\n            raise e\n        return False\n\n    def extract_image_data(self, outputs: Dict) -> Optional[Dict]:\n        final_image_data, temp_image_data = None, None\n        # Reverse the outputs dictionary items before iterating\n        for node_id, node_output in reversed(list(outputs.items())):\n            if \"ui\" in node_output and \"images\" in node_output.get(\"ui\", {}):\n                if node_output[\"ui\"][\"images\"]:\n                    final_image_data = node_output[\"ui\"][\"images\"][0]\n                    break\n            elif \"images\" in node_output and not temp_image_data:\n                if node_output[\"images\"]:\n                    temp_image_data = node_output[\"images\"][0]\n        return final_image_data if final_image_data else temp_image_data\n\n    async def queue_prompt(\n        self, session: aiohttp.ClientSession, workflow: Dict\n    ) -> Optional[str]:\n        payload = {\"prompt\": workflow, \"client_id\": self.client_id}\n        async with session.post(\n            f\"{self.valves.ComfyUI_Address}/prompt\", json=payload\n        ) as response:\n            response.raise_for_status()\n            data = await response.json()\n            return data.get(\"prompt_id\")\n\n    def parse_input(\n        self, messages: List[Dict[str, str]]\n    ) -> (Optional[str], Optional[str]):\n\n        user_message_item = get_last_user_message_item(messages)\n        if not user_message_item:\n            return None, None\n        prompt, image_url = \"\", None\n        content = user_message_item.get(\"content\")\n        print(str(content)[:200])\n        print(str(content)[::200])\n        if isinstance(content, list):\n            for part in content:\n                if part.get(\"type\") == \"text\":\n                    prompt += part.get(\"text\", \"\")\n                elif part.get(\"type\") == \"image_url\" and part.get(\"image_url\", {}).get(\n                    \"url\"\n                ):\n                    image_url = part[\"image_url\"][\"url\"]\n        elif isinstance(content, str):\n            prompt = content\n        if not image_url and user_message_item.get(\"images\"):\n            image_url = user_message_item[\"images\"][0]\n        base64_image = (\n            image_url.split(\"base64,\", 1)[1]\n            if image_url and \"base64,\" in image_url\n            else None\n        )\n        return prompt.strip(), base64_image\n\n    async def enhance_prompt(self, prompt, image, user, request, event_emitter):\n\n        await self.emit_status(event_emitter, \"info\", f\"Enhancing the prompt...\")\n        payload = {\n            \"model\": self.valves.vision_model_id,\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": self.valves.enhancer_system_prompt,\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": f\"Enhance the given user prompt based on the given image: {prompt}, provide only the enhnaced AI image edit prompt with no explanations\",\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n                        },\n                    ],\n                },\n            ],\n            \"stream\": False,\n        }\n\n        response = await generate_chat_completion(request, payload, user)\n        await self.emit_status(event_emitter, \"info\", f\"Prompt enhanced\")\n        enhanced_prompt = response[\"choices\"][0][\"message\"][\"content\"]\n        enhanced_prompt_message = f\"<details>\\n<summary>Enhanced Prompt</summary>\\n{enhanced_prompt}\\n\\n---\\n\\n</details>\"\n        await event_emitter(\n            {\n                \"type\": \"message\",\n                \"data\": {\n                    \"content\": enhanced_prompt_message,\n                },\n            }\n        )\n        return enhanced_prompt\n\n    async def pipe(\n        self,\n        body: dict,\n        __user__: dict,\n        __event_emitter__: Callable,\n        __request__=None,\n        __task__=None,\n    ) -> dict:\n        self.__event_emitter__ = __event_emitter__\n        self.__request__ = __request__\n        self.__user__ = Users.get_user_by_id(__user__[\"id\"])\n        messages = body.get(\"messages\", [])\n        prompt, base64_image = self.parse_input(messages)\n        if self.valves.enhance_prompt:\n            prompt = await self.enhance_prompt(\n                prompt,\n                base64_image,\n                self.__user__,\n                self.__request__,\n                self.__event_emitter__,\n            )\n\n        if __task__ and __task__ != TASKS.DEFAULT:\n            if self.valves.vision_model_id:\n                response = await generate_chat_completion(\n                    self.__request__,\n                    {\n                        \"model\": self.valves.vision_model_id,\n                        \"messages\": body.get(\"messages\"),\n                        \"stream\": False,\n                    },\n                    user=self.__user__,\n                )\n                return f\"{name}: {response['choices'][0]['message']['content']}\"\n            return f\"{name}: Edited Image!\"\n\n        if self.valves.unload_ollama_models:\n            await self.emit_status(\n                self.__event_emitter__, \"info\", \"Unloading Ollama models...\"\n            )\n            unload_all_models(api_url=self.valves.ollama_url)\n\n        if not base64_image:\n            await self.emit_status(\n                self.__event_emitter__,\n                \"error\",\n                \"No valid image provided. Please upload an image.\",\n                done=True,\n            )\n            return body\n\n        try:\n            workflow = json.loads(self.valves.ComfyUI_Workflow_JSON)\n        except json.JSONDecodeError:\n            await self.emit_status(\n                self.__event_emitter__,\n                \"error\",\n                \"Invalid JSON in the ComfyUI_Workflow_JSON valve.\",\n                done=True,\n            )\n            return body\n\n        http_api_url = self.valves.ComfyUI_Address.rstrip(\"/\")\n        ws_api_url = f\"{'ws' if not http_api_url.startswith('https') else 'wss'}://{http_api_url.split('://', 1)[-1]}/ws\"\n\n        prompt_node, image_node, seed_node = (\n            self.valves.Prompt_Node_ID,\n            self.valves.Image_Node_ID,\n            self.valves.Seed_Node_ID,\n        )\n        workflow[prompt_node][\"inputs\"][\"prompt\"] = (\n            prompt if prompt else \"A beautiful, high-quality image\"\n        )\n        workflow[image_node][\"inputs\"][\"base64_data\"] = base64_image\n        workflow[seed_node][\"inputs\"][\"seed\"] = random.randint(0, 2**32 - 1)\n\n        try:\n            async with aiohttp.ClientSession() as session:\n                prompt_id = await self.queue_prompt(session, workflow)\n                if not prompt_id:\n                    await self.emit_status(\n                        self.__event_emitter__,\n                        \"error\",\n                        \"Failed to queue prompt in ComfyUI.\",\n                        done=True,\n                    )\n                    return body\n\n                await self.emit_status(\n                    self.__event_emitter__,\n                    \"info\",\n                    f\"Workflow queued. Waiting for completion signal...\",\n                )\n                job_done = await self.wait_for_job_signal(\n                    ws_api_url, prompt_id, self.__event_emitter__\n                )\n\n                if not job_done:\n                    raise Exception(\n                        \"Did not receive a successful execution signal from ComfyUI.\"\n                    )\n\n                job_data = None\n                attempt = 0\n                while True:\n                    await asyncio.sleep(attempt + 1)\n                    logger.info(\n                        f\"Fetching history for prompt {prompt_id}, attempt {attempt + 1}...\"\n                    )\n                    async with session.get(\n                        f\"{http_api_url}/history/{prompt_id}\"\n                    ) as resp:\n                        if resp.status == 200:\n                            history = await resp.json()\n                            if prompt_id in history:\n                                job_data = history[prompt_id]\n                                break\n                    logger.warning(\n                        f\"Attempt {attempt + 1} to fetch history failed or was incomplete.\"\n                    )\n                    attempt += 1\n\n            if not job_data:\n                raise Exception(\n                    \"Failed to retrieve job data from history after multiple attempts.\"\n                )\n\n            logger.info(\n                f\"Received final job data from history: {json.dumps(job_data, indent=2)}\"\n            )\n            image_to_display = self.extract_image_data(job_data.get(\"outputs\", {}))\n\n            if image_to_display:\n\n                internal_image_url = f\"{http_api_url}/view?filename={image_to_display['filename']}&subfolder={image_to_display.get('subfolder', '')}&type={image_to_display.get('type', 'output')}\"\n                await self.emit_status(\n                    self.__event_emitter__,\n                    \"info\",\n                    f\"Downloading generated image from ComfyUI...\",\n                )\n\n                loop = asyncio.get_event_loop()\n                http_response = await loop.run_in_executor(\n                    None, requests.get, internal_image_url\n                )\n                http_response.raise_for_status()\n                image_data = http_response.content\n                content_type = http_response.headers.get(\"content-type\", \"image/png\")\n\n                await self.emit_status(\n                    self.__event_emitter__, \"info\", f\"Embedding image into chat...\"\n                )\n\n                public_image_url = self._save_image_and_get_public_url(\n                    request=self.__request__,\n                    image_data=image_data,\n                    content_type=content_type,\n                    user=self.__user__,\n                )\n\n                response_content = f\"Here is the edited image:\\n\\n![Generated Image]({public_image_url})\"\n\n                await self.__event_emitter__(\n                    {\"type\": \"message\", \"data\": {\"content\": response_content}}\n                )\n                await self.emit_status(\n                    self.__event_emitter__,\n                    \"success\",\n                    \"Image processed successfully!\",\n                    done=True,\n                )\n\n                body[\"messages\"].append(\n                    {\"role\": \"assistant\", \"content\": response_content}\n                )\n                return body\n\n            else:\n                await self.emit_status(\n                    self.__event_emitter__,\n                    \"error\",\n                    \"Execution finished, but no image was found in the output. Please check the workflow.\",\n                    done=True,\n                )\n\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred in pipe: {e}\", exc_info=True)\n            await self.emit_status(\n                self.__event_emitter__,\n                \"error\",\n                f\"An unexpected error occurred: {str(e)}\",\n                done=True,\n            )\n\n        return body\n","meta":{"description":"Pipe to generate Image to Image edits using the comfyui backend, defaults to a workflow using flux kontext","manifest":{}},"is_active":true,"is_global":false,"updated_at":1758088176,"created_at":1757994961}]